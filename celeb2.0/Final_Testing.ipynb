{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.cuda.set_device('cuda:0')\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# print(device)\n",
    "\n",
    "# pip install torch-summary\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "\n",
    "# import torchsummary\n",
    "import torchvision as tv\n",
    "from torchvision import transforms, datasets\n",
    "from torchvision.transforms import v2\n",
    "\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision.utils import make_grid\n",
    "\n",
    "from torch.distributions.normal import Normal\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torch.nn as nn\n",
    "from torch.nn import ReLU\n",
    "from torch.optim.lr_scheduler import _LRScheduler\n",
    "\n",
    "from time import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class CelebAEncoder(nn.Module):\n",
    "    def __init__(self, latent_dim=1024):\n",
    "        super(CelebAEncoder, self).__init__()\n",
    "        # Four 2D convolutional layers.\n",
    "        self.conv1 = nn.Conv2d(3, 2048, kernel_size=3, stride=2, padding=1)\n",
    "        self.conv2 = nn.Conv2d(2048, 1024, kernel_size=3, stride=2, padding=1)\n",
    "        self.conv3 = nn.Conv2d(1024, 512, kernel_size=5, stride=2, padding=2)\n",
    "        self.conv4 = nn.Conv2d(512, 256, kernel_size=5, stride=2, padding=2)\n",
    "        # For a 64×64 input, after 4 conv layers the feature map becomes 4×4.\n",
    "        self.fc = nn.Linear(256 * 4 * 4, 2048)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))   # [B, 2048, 32, 32]\n",
    "        x = F.relu(self.conv2(x))   # [B, 1024, 16, 16]\n",
    "        x = F.relu(self.conv3(x))   # [B, 512, 8, 8]\n",
    "        x = F.relu(self.conv4(x))   # [B, 256, 4, 4]\n",
    "        x = x.view(x.size(0), -1)     # Flatten\n",
    "        x = self.fc(x)              # [B, 2048]\n",
    "        # Split into two halves: one for μ and one for log-σ (pre-softplus)\n",
    "        mu, log_sigma = torch.chunk(x, 2, dim=1)\n",
    "        # Ensure σ > 0 using softplus.\n",
    "        sigma = F.softplus(log_sigma)\n",
    "        return mu, sigma\n",
    "\n",
    "class CelebADecoder(nn.Module):\n",
    "    def __init__(self, latent_dim=1024):\n",
    "        super(CelebADecoder, self).__init__()\n",
    "        # Transform the latent vector into a seed feature map.\n",
    "        self.fc = nn.Linear(latent_dim, 2048 * 4 * 4)\n",
    "        # Four transposed convolutional layers.\n",
    "        self.deconv1 = nn.ConvTranspose2d(2048, 1024, kernel_size=5, stride=2,\n",
    "                                          padding=2, output_padding=1)\n",
    "        self.deconv2 = nn.ConvTranspose2d(1024, 512, kernel_size=5, stride=2,\n",
    "                                          padding=2, output_padding=1)\n",
    "        self.deconv3 = nn.ConvTranspose2d(512, 256, kernel_size=3, stride=2,\n",
    "                                          padding=1, output_padding=1)\n",
    "        self.deconv4 = nn.ConvTranspose2d(256, 3, kernel_size=3, stride=2,\n",
    "                                          padding=1, output_padding=1)\n",
    "    \n",
    "    def forward(self, z):\n",
    "        x = self.fc(z)\n",
    "        x = x.view(-1, 2048, 4, 4)\n",
    "        x = F.relu(self.deconv1(x))\n",
    "        x = F.relu(self.deconv2(x))\n",
    "        x = F.relu(self.deconv3(x))\n",
    "        # Use sigmoid to map the output to [0, 1].\n",
    "        x = torch.sigmoid(self.deconv4(x))\n",
    "        return x\n",
    "\n",
    "class CelebAVAE(nn.Module):\n",
    "    def __init__(self, latent_dim=1024):\n",
    "        super(CelebAVAE, self).__init__()\n",
    "        self.encoder = CelebAEncoder(latent_dim)\n",
    "        self.decoder = CelebADecoder(latent_dim)\n",
    "    \n",
    "    def reparameterize(self, mu, sigma):\n",
    "        eps = torch.randn_like(sigma)\n",
    "        return mu + eps * sigma\n",
    "    \n",
    "    def forward(self, x):\n",
    "        mu, sigma = self.encoder(x)\n",
    "        z = self.reparameterize(mu, sigma)\n",
    "        recon = self.decoder(z)\n",
    "        return recon, mu, sigma\n",
    "\n",
    "#########################################\n",
    "# Loss Function for the VAE\n",
    "#########################################\n",
    "\n",
    "def loss_function(recon_x, x, mu, sigma):\n",
    "    # Reconstruction loss (BCE) summed over all pixels.\n",
    "    BCE = F.binary_cross_entropy(recon_x, x, reduction='sum')\n",
    "    # KL divergence between the approximate posterior and the unit Gaussian.\n",
    "    KL = -0.5 * torch.sum(1 + 2 * torch.log(sigma) - mu.pow(2) - sigma.pow(2))\n",
    "    return BCE + KL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class ActorNetwork(nn.Module):\n",
    "    def __init__(self, z_dim, attr_dim=None):\n",
    "        \"\"\"\n",
    "        If attr_dim is provided, the network is conditioned on attribute labels.\n",
    "        \"\"\"\n",
    "        super(ActorNetwork, self).__init__()\n",
    "        self.z_dim = z_dim\n",
    "        self.use_attr = attr_dim is not None\n",
    "        input_dim = z_dim\n",
    "        if self.use_attr:\n",
    "            # Map attribute labels to a 2048-dimensional embedding.\n",
    "            self.attr_fc = nn.Linear(attr_dim, 2048)\n",
    "            input_dim += 2048\n",
    "\n",
    "        # Four fully connected layers with 2048 outputs each.\n",
    "        self.fc1 = nn.Linear(input_dim, 2048)\n",
    "        self.fc2 = nn.Linear(2048, 2048)\n",
    "        self.fc3 = nn.Linear(2048, 2048)\n",
    "        self.fc4 = nn.Linear(2048, 2048)\n",
    "        # Final layer produces 2*z_dim outputs (to split into δz and gate logits)\n",
    "        self.fc_out = nn.Linear(2048, 2 * z_dim)\n",
    "    \n",
    "    def forward(self, z, y=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            z: latent vector [batch, z_dim]\n",
    "            y: attribute labels [batch, attr_dim] (optional)\n",
    "        Returns:\n",
    "            Transformed z'\n",
    "        \"\"\"\n",
    "        # Keep a copy of the original z for the residual update.\n",
    "        z_orig = z\n",
    "        if self.use_attr and y is not None:\n",
    "            y_emb = F.relu(self.attr_fc(y))\n",
    "            x = torch.cat([z, y_emb], dim=1)\n",
    "        else:\n",
    "            x = z\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = F.relu(self.fc4(x))\n",
    "        x = self.fc_out(x)\n",
    "        delta_z, gate_logits = torch.chunk(x, 2, dim=1)\n",
    "        gates = torch.sigmoid(gate_logits)\n",
    "        # Compute the updated latent vector:\n",
    "        z_transformed = (1 - gates) * z_orig + gates * delta_z\n",
    "        return z_transformed\n",
    "\n",
    "\n",
    "class CriticNetwork(nn.Module):\n",
    "    def __init__(self, z_dim, attr_dim=None):\n",
    "        \"\"\"\n",
    "        If attr_dim is provided, the critic is conditioned on attribute labels.\n",
    "        \"\"\"\n",
    "        super(CriticNetwork, self).__init__()\n",
    "        self.z_dim = z_dim\n",
    "        self.use_attr = attr_dim is not None\n",
    "        input_dim = z_dim\n",
    "        if self.use_attr:\n",
    "            self.attr_fc = nn.Linear(attr_dim, 2048)\n",
    "            input_dim += 2048\n",
    "\n",
    "        # Four fully connected layers with 2048 outputs each.\n",
    "        self.fc1 = nn.Linear(input_dim, 2048)\n",
    "        self.fc2 = nn.Linear(2048, 2048)\n",
    "        self.fc3 = nn.Linear(2048, 2048)\n",
    "        self.fc4 = nn.Linear(2048, 2048)\n",
    "        # Final layer produces a single output.\n",
    "        self.fc_out = nn.Linear(2048, 1)\n",
    "    \n",
    "    def forward(self, z, y=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            z: latent vector [batch, z_dim]\n",
    "            y: attribute labels [batch, attr_dim] (optional)\n",
    "        Returns:\n",
    "            Critic score in [0, 1]\n",
    "        \"\"\"\n",
    "        if self.use_attr and y is not None:\n",
    "            y_emb = F.relu(self.attr_fc(y))\n",
    "            x = torch.cat([z, y_emb], dim=1)\n",
    "        else:\n",
    "            x = z\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = F.relu(self.fc4(x))\n",
    "        x = self.fc_out(x)\n",
    "        output = torch.sigmoid(x)\n",
    "        return output\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1786056/2694883179.py:15: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  actor.load_state_dict(torch.load('actor_model.pt', map_location=device))\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for ActorNetwork:\n\tsize mismatch for attr_fc.weight: copying a param with shape torch.Size([2048, 10]) from checkpoint, the shape in current model is torch.Size([2048, 40]).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 15\u001b[0m\n\u001b[1;32m     12\u001b[0m vae \u001b[38;5;241m=\u001b[39m CelebAVAE()\u001b[38;5;241m.\u001b[39mto(device)  \u001b[38;5;66;03m# This is assumed to convert latent codes to images\u001b[39;00m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# Load the pre-saved weights.\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m \u001b[43mactor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_state_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mactor_model.pt\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmap_location\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     16\u001b[0m critic\u001b[38;5;241m.\u001b[39mload_state_dict(torch\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcritic_model.pt\u001b[39m\u001b[38;5;124m'\u001b[39m, map_location\u001b[38;5;241m=\u001b[39mdevice))\n\u001b[1;32m     17\u001b[0m vae\u001b[38;5;241m.\u001b[39mload_state_dict(torch\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mceleba_vae.pth\u001b[39m\u001b[38;5;124m'\u001b[39m, map_location\u001b[38;5;241m=\u001b[39mdevice))  \u001b[38;5;66;03m# adjust the filename as needed\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/latentconst/lib/python3.9/site-packages/torch/nn/modules/module.py:2584\u001b[0m, in \u001b[0;36mModule.load_state_dict\u001b[0;34m(self, state_dict, strict, assign)\u001b[0m\n\u001b[1;32m   2576\u001b[0m         error_msgs\u001b[38;5;241m.\u001b[39minsert(\n\u001b[1;32m   2577\u001b[0m             \u001b[38;5;241m0\u001b[39m,\n\u001b[1;32m   2578\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMissing key(s) in state_dict: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   2579\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m missing_keys)\n\u001b[1;32m   2580\u001b[0m             ),\n\u001b[1;32m   2581\u001b[0m         )\n\u001b[1;32m   2583\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(error_msgs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m-> 2584\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m   2585\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError(s) in loading state_dict for \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   2586\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(error_msgs)\n\u001b[1;32m   2587\u001b[0m         )\n\u001b[1;32m   2588\u001b[0m     )\n\u001b[1;32m   2589\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _IncompatibleKeys(missing_keys, unexpected_keys)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for ActorNetwork:\n\tsize mismatch for attr_fc.weight: copying a param with shape torch.Size([2048, 10]) from checkpoint, the shape in current model is torch.Size([2048, 40])."
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Hyperparameters and device settings\n",
    "latent_dim = 1024\n",
    "attr_dim = 40      # e.g., if you have 40 attribute values per sample\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Instantiate the networks and move them to the device.\n",
    "actor = ActorNetwork(z_dim=latent_dim, attr_dim=attr_dim).to(device)\n",
    "critic = CriticNetwork(z_dim=latent_dim, attr_dim=attr_dim).to(device)\n",
    "vae = CelebAVAE().to(device)  # This is assumed to convert latent codes to images\n",
    "\n",
    "# Load the pre-saved weights.\n",
    "actor.load_state_dict(torch.load('actor_model.pt', map_location=device))\n",
    "critic.load_state_dict(torch.load('critic_model.pt', map_location=device))\n",
    "vae.load_state_dict(torch.load('celeba_vae.pth', map_location=device))  # adjust the filename as needed\n",
    "\n",
    "decoder = vae.decoder\n",
    "\n",
    "# Set the models to evaluation mode.\n",
    "actor.eval()\n",
    "critic.eval()\n",
    "decoder.eval()\n",
    "\n",
    "# data = pd.read_csv(\"./data/list_attr_celeba.csv\").iloc[0,1:]\n",
    "# data = np.array(data.replace(-1,0).values)\n",
    "# print(data)\n",
    "\n",
    "# Create a sample latent vector and an attribute vector.\n",
    "# z_latent is a tensor of shape [1, latent_dim], and attributes is [1, attr_dim].\n",
    "z_latent = torch.randn(1, latent_dim).to(device)\n",
    "attributes = torch.zeros(1, attr_dim).to(device)  \n",
    "# attributes = torch.tensor(data).to(device)\n",
    "\n",
    "attributes[0][14] = 1\n",
    "\n",
    "# Use the actor to transform the latent vector.\n",
    "with torch.no_grad():\n",
    "    z_transformed = actor(z_latent, attributes)\n",
    "\n",
    "# Feed the transformed latent vector to the CelebA decoder to generate an image.\n",
    "with torch.no_grad():\n",
    "    generated_img = decoder(z_transformed)  # Expected output shape: [1, 3, H, W]\n",
    "\n",
    "# Process the generated image for plotting.\n",
    "# Remove the batch dimension and move to CPU.\n",
    "image = generated_img.squeeze(0).cpu()  # shape: [3, H, W]\n",
    "\n",
    "# Plot the image using matplotlib.\n",
    "plt.figure(figsize=(6, 6))\n",
    "# Convert from [C, H, W] to [H, W, C] for imshow.\n",
    "plt.imshow(image.permute(1, 2, 0).numpy())\n",
    "plt.axis('off')\n",
    "plt.title(\"Generated CelebA Image\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "latentconst",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
