{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.models import Inception_V3_Weights\n",
    "weights = Inception_V3_Weights.DEFAULT\n",
    "ans = torch.topk(prediction,1)[1][0][0]\n",
    "print(ans)\n",
    "print(weights.meta['categories'][ans])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2003897/360438095.py:30: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  x1 = int(annot[0][0])\n",
      "/tmp/ipykernel_2003897/360438095.py:31: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  y1 = int(annot[1][0])\n",
      "/tmp/ipykernel_2003897/360438095.py:32: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  x2 = int(annot[2][0])\n",
      "/tmp/ipykernel_2003897/360438095.py:33: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  y2 = int(annot[3][0])\n"
     ]
    }
   ],
   "source": [
    "from scipy.io import loadmat\n",
    "from PIL import Image\n",
    "import os\n",
    "\n",
    "# Load the .mat file\n",
    "data = loadmat('../dataset/car_devkit/devkit/cars_train_annos.mat')\n",
    "\n",
    "\n",
    "img_dir = '../dataset/cars_train/cars_train'\n",
    "filenames = [os.path.join(img_dir, fname) \n",
    "                          for fname in os.listdir(img_dir) \n",
    "                          if fname.lower().endswith(('.jpg', '.jpeg', '.png'))]\n",
    "\n",
    "# indices = np.random.randint(0,len(filenames),64)\n",
    "# print(filenames)\n",
    "\n",
    "X = []\n",
    "Y = {}\n",
    "dominant_colors = []\n",
    "results = []\n",
    "centres = []\n",
    "counts = []\n",
    "for i in range(64):\n",
    "    # Open the image using PIL\n",
    "    image = Image.open(filenames[i])\n",
    "\n",
    "    # Extract the index and annotation values (adjust this to your data format)\n",
    "    index = int(filenames[i].split('/')[-1].split('.')[0])\n",
    "    annot = data['annotations'][0][index-1]\n",
    "    x1 = int(annot[0][0])\n",
    "    y1 = int(annot[1][0])\n",
    "    x2 = int(annot[2][0])\n",
    "    y2 = int(annot[3][0])\n",
    "\n",
    "    # Crop the image using PIL's crop method.\n",
    "    # Note: PIL's crop expects (left, upper, right, lower)\n",
    "    cropped_img = image.crop((x1, y1, x2, y2))\n",
    "    \n",
    "    # Append the resulting NumPy array to your list\n",
    "    X.append(cropped_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import dnnlib\n",
    "import legacy\n",
    "from torchvision.utils import save_image\n",
    "from torchvision import models\n",
    "\n",
    "# --------------------------\n",
    "# Device Setup\n",
    "# --------------------------\n",
    "device = torch.device(\"cuda:2\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# --------------------------\n",
    "# Load Pretrained StyleGAN2 Network\n",
    "# --------------------------\n",
    "network_pkl = \"http://d36zk2xti64re0.cloudfront.net/stylegan2/networks/stylegan2-car-config-e.pkl\"\n",
    "with dnnlib.util.open_url(network_pkl) as f:\n",
    "    G = legacy.load_network_pkl(f)['G_ema'].to(device)\n",
    "G.eval()\n",
    "for param in G.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# --------------------------\n",
    "# Load Pretrained InceptionV3 Network\n",
    "# --------------------------\n",
    "inception = models.inception_v3(pretrained=True).to(device)\n",
    "inception.eval()\n",
    "for param in inception.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# --------------------------\n",
    "# Define the Latent VAE for generating latent vectors\n",
    "# --------------------------\n",
    "class LatentVAE(nn.Module):\n",
    "    def __init__(self, input_dim=512, hidden_dim=512, latent_dim=512):\n",
    "        super(LatentVAE, self).__init__()\n",
    "        # Encoder layers\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.fc_mu = nn.Linear(hidden_dim, latent_dim)\n",
    "        self.fc_logvar = nn.Linear(hidden_dim, latent_dim)\n",
    "        # Decoder layers\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(latent_dim, hidden_dim),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(hidden_dim, input_dim)\n",
    "        )\n",
    "    \n",
    "    def encode(self, x):\n",
    "        h = F.relu(self.fc1(x))\n",
    "        mu = self.fc_mu(h)\n",
    "        logvar = self.fc_logvar(h)\n",
    "        return mu, logvar\n",
    "    \n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps * std\n",
    "    \n",
    "    def decode(self, z):\n",
    "        return self.decoder(z)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        mu, logvar = self.encode(x)\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        x_recon = self.decode(z)\n",
    "        return x_recon, mu, logvar\n",
    "\n",
    "latent_vae = LatentVAE().to(device)\n",
    "optimizer = optim.Adam(latent_vae.parameters(), lr=1e-4)\n",
    "\n",
    "# --------------------------\n",
    "# Preprocessing Transforms\n",
    "# --------------------------\n",
    "preprocess_img = transforms.Compose([\n",
    "    transforms.Resize((299, 299)),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "preprocess_resize = transforms.Resize((299, 299))  # For generated images\n",
    "\n",
    "# --------------------------\n",
    "# Helper Functions\n",
    "# --------------------------\n",
    "def get_layer_activation(model, input_tensor, layer_name, detach=True):\n",
    "    \"\"\"\n",
    "    Extract activations from a specified layer.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    activations = {}\n",
    "    def hook(module, input, output):\n",
    "        activations[layer_name] = output.detach() if detach else output\n",
    "    target_module = None\n",
    "    for name, module in model.named_modules():\n",
    "        if name == layer_name:\n",
    "            target_module = module\n",
    "            break\n",
    "    if target_module is None:\n",
    "        raise ValueError(f\"Layer '{layer_name}' not found in model.\")\n",
    "    hook_handle = target_module.register_forward_hook(hook)\n",
    "    _ = model(input_tensor)\n",
    "    hook_handle.remove()\n",
    "    return activations[layer_name]\n",
    "\n",
    "def gram_matrix(features):\n",
    "    b, c, h, w = features.size()\n",
    "    features = features.view(b, c, h * w)\n",
    "    gram = torch.bmm(features, features.transpose(1, 2))\n",
    "    return gram / (c * h * w)\n",
    "\n",
    "def compute_content_loss(target, generated):\n",
    "    return F.mse_loss(generated, target)\n",
    "\n",
    "# Optionally, specify layers for style and content loss computation\n",
    "selected_layers_style = ['Conv2d_1a_3x3', 'Mixed_5c', 'Mixed_6e']\n",
    "selected_layers_content = ['Conv2d_1a_3x3', 'Mixed_5c', 'Mixed_6e']\n",
    "\n",
    "# --------------------------\n",
    "# Prepare the Target Image\n",
    "# --------------------------\n",
    "# Note: Replace this dummy image with your actual dataset image.\n",
    "# dummy_image = X[15]  # Dummy image\n",
    "# target_tensor = preprocess_img(dummy_image).unsqueeze(0).to(device)\n",
    "\n",
    "# plt.imshow(target_tensor.cpu().squeeze().permute(1,2,0))\n",
    "# plt.title(\"Target Image\")\n",
    "# plt.axis('off')\n",
    "# plt.show()\n",
    "\n",
    "# --------------------------\n",
    "# Define Loss Function for Classification Reward\n",
    "# --------------------------\n",
    "# Use Binary Cross Entropy (BCE) loss where a low loss (reward) is assigned\n",
    "# if InceptionV3 classifies the generated image as a sports car (label 847).\n",
    "bce_loss = nn.BCELoss()\n",
    "\n",
    "\n",
    "\n",
    "import torch.nn.functional as F  # ensure F is imported for interpolation\n",
    "\n",
    "# --------------------------\n",
    "# Training Loop with Batch Size 32\n",
    "# --------------------------\n",
    "batch_size = 10\n",
    "num_steps = 10000\n",
    "for step in range(num_steps):\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # Generate a batch of random latent vectors and pass them through the VAE.\n",
    "    z = torch.randn(batch_size, 512).to(device)\n",
    "    latent_out, mu, logvar = latent_vae(z)\n",
    "    \n",
    "    # Use the decoded latent vectors with StyleGAN2's mapping network.\n",
    "    w = G.mapping(latent_out, None)\n",
    "    \n",
    "    # Generate a batch of images using StyleGAN2's synthesis network.\n",
    "    imgs = G.synthesis(w, noise_mode='const')\n",
    "    imgs = (imgs.clamp(-1, 1) + 1) / 2  # Normalize images to [0, 1]\n",
    "    \n",
    "    # Resize generated images to 299x299 for Inception using bilinear interpolation.\n",
    "    imgs_inp = F.interpolate(imgs, size=(299, 299), mode='bilinear', align_corners=False)\n",
    "    \n",
    "    # Forward pass through Inception to get predictions.\n",
    "    predictions = inception(imgs_inp)\n",
    "    pred_labels = predictions.argmax(dim=1)\n",
    "    target_labels = torch.full((batch_size,), 847, dtype=torch.long).to(device)  # Target label: 847\n",
    "    \n",
    "    # Compute classification loss using BCE.\n",
    "    # First, get binary predictions (1 if predicted label equals target label, else 0).\n",
    "    # print(pred_labels)\n",
    "    # break\n",
    "    y_pred = (pred_labels == target_labels).float()\n",
    "    classification_loss = nn.BCELoss()(y_pred, torch.ones_like(y_pred))\n",
    "    \n",
    "    # VAE Losses:\n",
    "    # Reconstruction loss: encouraging latent_out to be close to the original sampled z.\n",
    "    reconstruction_loss = F.mse_loss(latent_out, z)\n",
    "    # KL divergence: regularizing the latent space to be close to a standard normal distribution.\n",
    "    kl_divergence = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp()) / batch_size\n",
    "    \n",
    "    # Total loss: sum of classification, reconstruction, and KL divergence losses.\n",
    "    total_loss = classification_loss + reconstruction_loss + kl_divergence\n",
    "    \n",
    "    total_loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    # Periodically save the model and display the first generated image in the batch.\n",
    "    if step % 100 == 0:\n",
    "        torch.save(latent_vae.state_dict(), \"latent_vae.pth\")\n",
    "        print(f\"Step {step}: Total Loss = {total_loss.item():.4f}, \"\n",
    "              f\"Classification Loss = {classification_loss.item():.4f}, \"\n",
    "              f\"Reconstruction Loss = {reconstruction_loss.item():.4f}, \"\n",
    "              f\"KL Divergence = {kl_divergence.item():.4f}\")\n",
    "        \n",
    "        # Display the first image from the batch.\n",
    "        plt.imshow(imgs[0].detach().cpu().permute(1, 2, 0))\n",
    "        plt.title(f\"Generated Image at Step {step}\")\n",
    "        plt.axis('off')\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "latentconst",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
