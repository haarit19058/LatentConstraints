{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "torch.cuda.set_device(0)\n",
    "print(torch.cuda.current_device())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "UnidentifiedImageError",
     "evalue": "cannot identify image file <_io.BytesIO object at 0x7fcfd6188360>",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnidentifiedImageError\u001b[0m                    Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 9\u001b[0m\n\u001b[1;32m      7\u001b[0m url \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttps://example.com/sample_image.jpg\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m      8\u001b[0m response \u001b[38;5;241m=\u001b[39m requests\u001b[38;5;241m.\u001b[39mget(url)\n\u001b[0;32m----> 9\u001b[0m img \u001b[38;5;241m=\u001b[39m \u001b[43mImage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mBytesIO\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcontent\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# Define preprocessing steps (resize, crop, convert to tensor, normalize)\u001b[39;00m\n\u001b[1;32m     12\u001b[0m preprocess \u001b[38;5;241m=\u001b[39m transforms\u001b[38;5;241m.\u001b[39mCompose([\n\u001b[1;32m     13\u001b[0m     transforms\u001b[38;5;241m.\u001b[39mResize(\u001b[38;5;241m256\u001b[39m),\n\u001b[1;32m     14\u001b[0m     transforms\u001b[38;5;241m.\u001b[39mCenterCrop(\u001b[38;5;241m224\u001b[39m),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     17\u001b[0m                          std\u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m0.229\u001b[39m, \u001b[38;5;241m0.224\u001b[39m, \u001b[38;5;241m0.225\u001b[39m])\n\u001b[1;32m     18\u001b[0m ])\n",
      "File \u001b[0;32m~/anaconda3/envs/latentconst/lib/python3.9/site-packages/PIL/Image.py:3536\u001b[0m, in \u001b[0;36mopen\u001b[0;34m(fp, mode, formats)\u001b[0m\n\u001b[1;32m   3534\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(message)\n\u001b[1;32m   3535\u001b[0m msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcannot identify image file \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m (filename \u001b[38;5;28;01mif\u001b[39;00m filename \u001b[38;5;28;01melse\u001b[39;00m fp)\n\u001b[0;32m-> 3536\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m UnidentifiedImageError(msg)\n",
      "\u001b[0;31mUnidentifiedImageError\u001b[0m: cannot identify image file <_io.BytesIO object at 0x7fcfd6188360>"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "from torchvision import transforms\n",
    "\n",
    "# URL of a sample image (ensure the image is appropriate for testing)\n",
    "url = 'https://example.com/sample_image.jpg'\n",
    "response = requests.get(url)\n",
    "img = Image.open(BytesIO(response.content))\n",
    "\n",
    "# Define preprocessing steps (resize, crop, convert to tensor, normalize)\n",
    "preprocess = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                         std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "input_tensor = preprocess(img)\n",
    "input_batch = input_tensor.unsqueeze(0)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========================================================================================\n",
      "Layer (type:depth-idx)                   Output Shape              Param #\n",
      "==========================================================================================\n",
      "├─Sequential: 1-1                        [-1, 512, 7, 7]           --\n",
      "|    └─Conv2d: 2-1                       [-1, 64, 224, 224]        1,792\n",
      "|    └─ReLU: 2-2                         [-1, 64, 224, 224]        --\n",
      "|    └─Conv2d: 2-3                       [-1, 64, 224, 224]        36,928\n",
      "|    └─ReLU: 2-4                         [-1, 64, 224, 224]        --\n",
      "|    └─MaxPool2d: 2-5                    [-1, 64, 112, 112]        --\n",
      "|    └─Conv2d: 2-6                       [-1, 128, 112, 112]       73,856\n",
      "|    └─ReLU: 2-7                         [-1, 128, 112, 112]       --\n",
      "|    └─Conv2d: 2-8                       [-1, 128, 112, 112]       147,584\n",
      "|    └─ReLU: 2-9                         [-1, 128, 112, 112]       --\n",
      "|    └─MaxPool2d: 2-10                   [-1, 128, 56, 56]         --\n",
      "|    └─Conv2d: 2-11                      [-1, 256, 56, 56]         295,168\n",
      "|    └─ReLU: 2-12                        [-1, 256, 56, 56]         --\n",
      "|    └─Conv2d: 2-13                      [-1, 256, 56, 56]         590,080\n",
      "|    └─ReLU: 2-14                        [-1, 256, 56, 56]         --\n",
      "|    └─Conv2d: 2-15                      [-1, 256, 56, 56]         590,080\n",
      "|    └─ReLU: 2-16                        [-1, 256, 56, 56]         --\n",
      "|    └─MaxPool2d: 2-17                   [-1, 256, 28, 28]         --\n",
      "|    └─Conv2d: 2-18                      [-1, 512, 28, 28]         1,180,160\n",
      "|    └─ReLU: 2-19                        [-1, 512, 28, 28]         --\n",
      "|    └─Conv2d: 2-20                      [-1, 512, 28, 28]         2,359,808\n",
      "|    └─ReLU: 2-21                        [-1, 512, 28, 28]         --\n",
      "|    └─Conv2d: 2-22                      [-1, 512, 28, 28]         2,359,808\n",
      "|    └─ReLU: 2-23                        [-1, 512, 28, 28]         --\n",
      "|    └─MaxPool2d: 2-24                   [-1, 512, 14, 14]         --\n",
      "|    └─Conv2d: 2-25                      [-1, 512, 14, 14]         2,359,808\n",
      "|    └─ReLU: 2-26                        [-1, 512, 14, 14]         --\n",
      "|    └─Conv2d: 2-27                      [-1, 512, 14, 14]         2,359,808\n",
      "|    └─ReLU: 2-28                        [-1, 512, 14, 14]         --\n",
      "|    └─Conv2d: 2-29                      [-1, 512, 14, 14]         2,359,808\n",
      "|    └─ReLU: 2-30                        [-1, 512, 14, 14]         --\n",
      "|    └─MaxPool2d: 2-31                   [-1, 512, 7, 7]           --\n",
      "├─AdaptiveAvgPool2d: 1-2                 [-1, 512, 7, 7]           --\n",
      "├─Sequential: 1-3                        [-1, 1000]                --\n",
      "|    └─Linear: 2-32                      [-1, 4096]                102,764,544\n",
      "|    └─ReLU: 2-33                        [-1, 4096]                --\n",
      "|    └─Dropout: 2-34                     [-1, 4096]                --\n",
      "|    └─Linear: 2-35                      [-1, 4096]                16,781,312\n",
      "|    └─ReLU: 2-36                        [-1, 4096]                --\n",
      "|    └─Dropout: 2-37                     [-1, 4096]                --\n",
      "|    └─Linear: 2-38                      [-1, 1000]                4,097,000\n",
      "==========================================================================================\n",
      "Total params: 138,357,544\n",
      "Trainable params: 138,357,544\n",
      "Non-trainable params: 0\n",
      "Total mult-adds (G): 15.61\n",
      "==========================================================================================\n",
      "Input size (MB): 0.57\n",
      "Forward/backward pass size (MB): 103.43\n",
      "Params size (MB): 527.79\n",
      "Estimated Total Size (MB): 631.80\n",
      "==========================================================================================\n",
      "==========================================================================================\n",
      "Layer (type:depth-idx)                   Output Shape              Param #\n",
      "==========================================================================================\n",
      "├─Sequential: 1-1                        [-1, 512, 7, 7]           --\n",
      "|    └─Conv2d: 2-1                       [-1, 64, 224, 224]        1,792\n",
      "|    └─ReLU: 2-2                         [-1, 64, 224, 224]        --\n",
      "|    └─Conv2d: 2-3                       [-1, 64, 224, 224]        36,928\n",
      "|    └─ReLU: 2-4                         [-1, 64, 224, 224]        --\n",
      "|    └─MaxPool2d: 2-5                    [-1, 64, 112, 112]        --\n",
      "|    └─Conv2d: 2-6                       [-1, 128, 112, 112]       73,856\n",
      "|    └─ReLU: 2-7                         [-1, 128, 112, 112]       --\n",
      "|    └─Conv2d: 2-8                       [-1, 128, 112, 112]       147,584\n",
      "|    └─ReLU: 2-9                         [-1, 128, 112, 112]       --\n",
      "|    └─MaxPool2d: 2-10                   [-1, 128, 56, 56]         --\n",
      "|    └─Conv2d: 2-11                      [-1, 256, 56, 56]         295,168\n",
      "|    └─ReLU: 2-12                        [-1, 256, 56, 56]         --\n",
      "|    └─Conv2d: 2-13                      [-1, 256, 56, 56]         590,080\n",
      "|    └─ReLU: 2-14                        [-1, 256, 56, 56]         --\n",
      "|    └─Conv2d: 2-15                      [-1, 256, 56, 56]         590,080\n",
      "|    └─ReLU: 2-16                        [-1, 256, 56, 56]         --\n",
      "|    └─MaxPool2d: 2-17                   [-1, 256, 28, 28]         --\n",
      "|    └─Conv2d: 2-18                      [-1, 512, 28, 28]         1,180,160\n",
      "|    └─ReLU: 2-19                        [-1, 512, 28, 28]         --\n",
      "|    └─Conv2d: 2-20                      [-1, 512, 28, 28]         2,359,808\n",
      "|    └─ReLU: 2-21                        [-1, 512, 28, 28]         --\n",
      "|    └─Conv2d: 2-22                      [-1, 512, 28, 28]         2,359,808\n",
      "|    └─ReLU: 2-23                        [-1, 512, 28, 28]         --\n",
      "|    └─MaxPool2d: 2-24                   [-1, 512, 14, 14]         --\n",
      "|    └─Conv2d: 2-25                      [-1, 512, 14, 14]         2,359,808\n",
      "|    └─ReLU: 2-26                        [-1, 512, 14, 14]         --\n",
      "|    └─Conv2d: 2-27                      [-1, 512, 14, 14]         2,359,808\n",
      "|    └─ReLU: 2-28                        [-1, 512, 14, 14]         --\n",
      "|    └─Conv2d: 2-29                      [-1, 512, 14, 14]         2,359,808\n",
      "|    └─ReLU: 2-30                        [-1, 512, 14, 14]         --\n",
      "|    └─MaxPool2d: 2-31                   [-1, 512, 7, 7]           --\n",
      "├─AdaptiveAvgPool2d: 1-2                 [-1, 512, 7, 7]           --\n",
      "├─Sequential: 1-3                        [-1, 1000]                --\n",
      "|    └─Linear: 2-32                      [-1, 4096]                102,764,544\n",
      "|    └─ReLU: 2-33                        [-1, 4096]                --\n",
      "|    └─Dropout: 2-34                     [-1, 4096]                --\n",
      "|    └─Linear: 2-35                      [-1, 4096]                16,781,312\n",
      "|    └─ReLU: 2-36                        [-1, 4096]                --\n",
      "|    └─Dropout: 2-37                     [-1, 4096]                --\n",
      "|    └─Linear: 2-38                      [-1, 1000]                4,097,000\n",
      "==========================================================================================\n",
      "Total params: 138,357,544\n",
      "Trainable params: 138,357,544\n",
      "Non-trainable params: 0\n",
      "Total mult-adds (G): 15.61\n",
      "==========================================================================================\n",
      "Input size (MB): 0.57\n",
      "Forward/backward pass size (MB): 103.43\n",
      "Params size (MB): 527.79\n",
      "Estimated Total Size (MB): 631.80\n",
      "==========================================================================================\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision.models as models\n",
    "from torchsummary import summary\n",
    "\n",
    "# Load a pretrained VGG16 model\n",
    "vgg16 = models.vgg16(weights=models.VGG16_Weights.DEFAULT)\n",
    "\n",
    "# Print model summary\n",
    "print(summary(vgg16, (3, 224, 224)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from PIL import Image\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.models as models\n",
    "\n",
    "def process(img):\n",
    "    # Define preprocessing steps: resize, crop, convert to tensor, and normalize.\n",
    "    preprocess = transforms.Compose([\n",
    "        transforms.Resize(256),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                             std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "    input_tensor = preprocess(img)\n",
    "    input_batch = input_tensor.unsqueeze(0)  # Create a mini-batch as expected by the model.\n",
    "    return input_batch\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/project/anaconda3/envs/latentconst/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/project/anaconda3/envs/latentconst/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "207\n",
      "794\n"
     ]
    }
   ],
   "source": [
    "# Load the image using PIL\n",
    "dog = Image.open('dog.jpg')\n",
    "\n",
    "# Preprocess the image\n",
    "dog = process(dog)\n",
    "\n",
    "# Load the pretrained VGG16 model and set it to evaluation mode\n",
    "vgg16 = models.vgg16(pretrained=True)\n",
    "vgg16.eval()\n",
    "\n",
    "# Forward pass: make sure to disable gradients for inference.\n",
    "with torch.no_grad():\n",
    "    output = vgg16(dog)\n",
    "\n",
    "# Print the index of the class with the highest score\n",
    "print(output.argmax().item())\n",
    "\n",
    "\n",
    "style = Image.open('style.jpg')\n",
    "style = process(style).to(device)\n",
    "vgg16 = models.vgg16(pretrained=True)\n",
    "vgg16.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    output = vgg16(style)   \n",
    "    \n",
    "print(output.argmax().item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_content(output, target):\n",
    "    return torch.nn.functional.mse_loss(output, target)\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def gram_matrix(feature_map):\n",
    "    \"\"\"\n",
    "    Compute the Gram matrix from a given feature map.\n",
    "    \n",
    "    Args:\n",
    "        feature_map (torch.Tensor): Tensor of shape (batch_size, channels, height, width).\n",
    "    \n",
    "    Returns:\n",
    "        torch.Tensor: Normalized Gram matrix of shape (batch_size, channels, channels).\n",
    "    \"\"\"\n",
    "    batch_size, channels, height, width = feature_map.size()\n",
    "    # Reshape feature_map to (batch_size, channels, height*width)\n",
    "    features = feature_map.view(batch_size, channels, height * width)\n",
    "    # Compute the Gram matrix using batch matrix multiplication\n",
    "    gram = torch.bmm(features, features.transpose(1, 2))\n",
    "    # Normalize the Gram matrix by the number of elements in each feature map\n",
    "    gram = gram / (channels * height * width)\n",
    "    return gram\n",
    "\n",
    "def style_loss(generated, target):\n",
    "    \"\"\"\n",
    "    Computes the style loss between generated image features and target style features.\n",
    "    \n",
    "    Args:\n",
    "        generated (torch.Tensor): Generated image features of shape (batch_size, channels, height, width).\n",
    "        target (torch.Tensor): Target style image features of shape (batch_size, channels, height, width).\n",
    "    \n",
    "    Returns:\n",
    "        torch.Tensor: Scalar representing the style loss.\n",
    "    \"\"\"\n",
    "    # Compute Gram matrices for both feature maps\n",
    "    gram_generated = gram_matrix(generated)\n",
    "    gram_target = gram_matrix(target)\n",
    "    # Compute the mean squared error between the Gram matrices\n",
    "    loss = F.mse_loss(gram_generated, gram_target)\n",
    "    return loss\n",
    " \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/project/anaconda3/envs/latentconst/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/project/anaconda3/envs/latentconst/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG19_Weights.IMAGENET1K_V1`. You can also use `weights=VGG19_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "/tmp/ipykernel_2170155/2828339652.py:88: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.mean = torch.tensor(mean).view(-1, 1, 1).to(device)\n",
      "/tmp/ipykernel_2170155/2828339652.py:89: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.std = torch.tensor(std).view(-1, 1, 1).to(device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting the style transfer...\n",
      "Iteration 50:\n",
      "Style Loss:  nan Content Loss:  nan\n",
      "\n",
      "Iteration 100:\n",
      "Style Loss:  nan Content Loss:  nan\n",
      "\n",
      "Iteration 150:\n",
      "Style Loss:  nan Content Loss:  nan\n",
      "\n",
      "Iteration 200:\n",
      "Style Loss:  nan Content Loss:  nan\n",
      "\n",
      "Iteration 250:\n",
      "Style Loss:  nan Content Loss:  nan\n",
      "\n",
      "Iteration 300:\n",
      "Style Loss:  nan Content Loss:  nan\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/project/anaconda3/envs/latentconst/lib/python3.9/site-packages/torchvision/transforms/functional.py:282: RuntimeWarning: invalid value encountered in cast\n",
      "  npimg = (npimg * 255).astype(np.uint8)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.models as models\n",
    "import copy\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# Set device: use GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Define the image size (smaller if no GPU)\n",
    "imsize = 512 if torch.cuda.is_available() else 128\n",
    "\n",
    "# Preprocessing: resize and convert to tensor\n",
    "loader = transforms.Compose([\n",
    "    transforms.Resize(imsize),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "def image_loader(image_path):\n",
    "    \"\"\"Load an image and convert it to a normalized torch tensor.\"\"\"\n",
    "    image = Image.open(image_path)\n",
    "    # Add batch dimension and move to device\n",
    "    image = loader(image).unsqueeze(0)\n",
    "    return image.to(device, torch.float)\n",
    "\n",
    "# Load the content and style images (ensure both images are the same size)\n",
    "content_img =dog.to(device)\n",
    "style_img = style.to(device)\n",
    "assert content_img.size() == style_img.size(), \"Content and style images must be of the same size.\"\n",
    "\n",
    "# Function to convert tensor to PIL image for visualization\n",
    "unloader = transforms.ToPILImage()\n",
    "def imshow(tensor, title=None):\n",
    "    image = tensor.cpu().clone()\n",
    "    image = image.squeeze(0)      # remove the batch dimension\n",
    "    image = unloader(image)\n",
    "    plt.imshow(image)\n",
    "    if title is not None:\n",
    "        plt.title(title)\n",
    "    plt.pause(0.001)\n",
    "\n",
    "# Define a module to compute content loss\n",
    "class ContentLoss(nn.Module):\n",
    "    def __init__(self, target):\n",
    "        super(ContentLoss, self).__init__()\n",
    "        # 'detach' the target content from the graph\n",
    "        self.target = target.detach()\n",
    "\n",
    "    def forward(self, input):\n",
    "        self.loss = nn.functional.mse_loss(input, self.target)\n",
    "        return input\n",
    "\n",
    "# Helper function: compute Gram matrix for style loss\n",
    "def gram_matrix(input):\n",
    "    a, b, c, d = input.size()  # a=batch size(=1)\n",
    "    features = input.view(a * b, c * d)  # reshape into two dimensions\n",
    "    G = torch.mm(features, features.t())\n",
    "    return G.div(a * b * c * d)\n",
    "\n",
    "# Define a module to compute style loss\n",
    "class StyleLoss(nn.Module):\n",
    "    def __init__(self, target_feature):\n",
    "        super(StyleLoss, self).__init__()\n",
    "        # compute the Gram matrix for target style and detach it\n",
    "        self.target = gram_matrix(target_feature).detach()\n",
    "\n",
    "    def forward(self, input):\n",
    "        G = gram_matrix(input)\n",
    "        self.loss = nn.functional.mse_loss(G, self.target)\n",
    "        return input\n",
    "\n",
    "# Load the pre-trained VGG19 model (we only need the features)\n",
    "cnn = models.vgg19(pretrained=True).features.to(device).eval()\n",
    "\n",
    "# Normalization mean and std for VGG networks\n",
    "cnn_normalization_mean = torch.tensor([0.485, 0.456, 0.406]).to(device)\n",
    "cnn_normalization_std = torch.tensor([0.229, 0.224, 0.225]).to(device)\n",
    "\n",
    "# Define a normalization module to use in our model\n",
    "class Normalization(nn.Module):\n",
    "    def __init__(self, mean, std):\n",
    "        super(Normalization, self).__init__()\n",
    "        # reshape mean and std for easy subtraction and division\n",
    "        self.mean = torch.tensor(mean).view(-1, 1, 1).to(device)\n",
    "        self.std = torch.tensor(std).view(-1, 1, 1).to(device)\n",
    "\n",
    "    def forward(self, img):\n",
    "        return (img - self.mean) / self.std\n",
    "\n",
    "# Layers to compute content and style losses from the VGG network\n",
    "content_layers_default = ['conv_4']\n",
    "style_layers_default = ['conv_1', 'conv_2', 'conv_3', 'conv_4', 'conv_5']\n",
    "\n",
    "def get_style_model_and_losses(cnn, normalization_mean, normalization_std,\n",
    "                               style_img, content_img,\n",
    "                               content_layers=content_layers_default,\n",
    "                               style_layers=style_layers_default):\n",
    "    # Copy the CNN to avoid modifying the original\n",
    "    cnn = copy.deepcopy(cnn)\n",
    "    normalization = Normalization(normalization_mean, normalization_std).to(device)\n",
    "\n",
    "    content_losses = []\n",
    "    style_losses = []\n",
    "\n",
    "    # Build a new sequential model with normalization as the first layer\n",
    "    model = nn.Sequential(normalization)\n",
    "\n",
    "    i = 0  # Increment for every convolutional layer\n",
    "    for layer in cnn.children():\n",
    "        if isinstance(layer, nn.Conv2d):\n",
    "            i += 1\n",
    "            name = 'conv_{}'.format(i)\n",
    "        elif isinstance(layer, nn.ReLU):\n",
    "            name = 'relu_{}'.format(i)\n",
    "            # Replace in-place ReLU to avoid modifying computation graph\n",
    "            layer = nn.ReLU(inplace=False)\n",
    "        elif isinstance(layer, nn.MaxPool2d):\n",
    "            name = 'pool_{}'.format(i)\n",
    "        elif isinstance(layer, nn.BatchNorm2d):\n",
    "            name = 'bn_{}'.format(i)\n",
    "        else:\n",
    "            raise RuntimeError('Unrecognized layer: {}'.format(layer.__class__.__name__))\n",
    "\n",
    "        model.add_module(name, layer)\n",
    "\n",
    "        if name in content_layers:\n",
    "            target = model(content_img).detach()\n",
    "            content_loss = ContentLoss(target)\n",
    "            model.add_module(\"content_loss_{}\".format(i), content_loss)\n",
    "            content_losses.append(content_loss)\n",
    "\n",
    "        if name in style_layers:\n",
    "            target_feature = model(style_img).detach()\n",
    "            style_loss = StyleLoss(target_feature)\n",
    "            model.add_module(\"style_loss_{}\".format(i), style_loss)\n",
    "            style_losses.append(style_loss)\n",
    "\n",
    "    # Trim off layers after the last content or style loss module\n",
    "    for i in range(len(model) - 1, -1, -1):\n",
    "        if isinstance(model[i], ContentLoss) or isinstance(model[i], StyleLoss):\n",
    "            break\n",
    "    model = model[:(i + 1)]\n",
    "\n",
    "    return model, style_losses, content_losses\n",
    "\n",
    "# Get our model with loss layers\n",
    "model, style_losses, content_losses = get_style_model_and_losses(cnn,\n",
    "    cnn_normalization_mean, cnn_normalization_std, style_img, content_img)\n",
    "\n",
    "# Initialize the input image as a clone of the content image\n",
    "input_img = content_img.clone()\n",
    "\n",
    "# Choose LBFGS optimizer for the input image\n",
    "optimizer = optim.LBFGS([input_img.requires_grad_()])\n",
    "\n",
    "# NST hyperparameters: number of iterations, style weight, and content weight\n",
    "num_steps = 300\n",
    "style_weight = 1e6\n",
    "content_weight = 1\n",
    "\n",
    "print(\"Starting the style transfer...\")\n",
    "run = [0]\n",
    "while run[0] <= num_steps:\n",
    "\n",
    "    def closure():\n",
    "        # Clamp the input image to [0, 1]\n",
    "        input_img.data.clamp_(0, 1)\n",
    "        optimizer.zero_grad()\n",
    "        model(input_img)\n",
    "        style_score = 0\n",
    "        content_score = 0\n",
    "\n",
    "        # Sum up the style and content losses\n",
    "        for sl in style_losses:\n",
    "            style_score += sl.loss\n",
    "        for cl in content_losses:\n",
    "            content_score += cl.loss\n",
    "\n",
    "        # Total loss is weighted sum of style and content losses\n",
    "        loss = style_weight * style_score + content_weight * content_score\n",
    "        loss.backward()\n",
    "\n",
    "        run[0] += 1\n",
    "        if run[0] % 50 == 0:\n",
    "            print(\"Iteration {}:\".format(run[0]))\n",
    "            print('Style Loss: {:4f} Content Loss: {:4f}'.format(\n",
    "                style_score.item(), content_score.item()))\n",
    "            print()\n",
    "        return loss\n",
    "\n",
    "    optimizer.step(closure)\n",
    "\n",
    "# Clamp the final output and display\n",
    "input_img.data.clamp_(0, 1)\n",
    "plt.figure()\n",
    "imshow(input_img, title='Output Image')\n",
    "plt.ioff()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "latentconst",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
